{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHGXD3YsK+3gKwyZxt+d9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityav1810/Implementing-Diffusion-Models/blob/main/Basic_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFXGQeG0E554"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format = \"%(asctime)s - %(levelname)s: %(message)s\",level = logging.INFO,datefmt=\"%I:%M:%S\")"
      ],
      "metadata": {
        "id": "tlKimiHrFTT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion:\n",
        "  '''\n",
        "  Class to setup utils and necessary functions needed to build a basic diffusion model\n",
        "  Params are the same as presented in the original paper(https://arxiv.org/abs/2006.11239)\n",
        "\n",
        "  '''\n",
        "  def __init(self,noise_steps = 1000,beta_start = 1e-4,beta_end = 0.02,img_size = 64,device=\"cuda\"):\n",
        "    '''\n",
        "    Initialise variables generate noise\n",
        "    beta is used to generate noise; alpha = 1-beta\n",
        "\n",
        "    '''\n",
        "    self.noise_steps = noise_steps\n",
        "    self.beta_start = beta_start\n",
        "    self.beta_end = beta_end\n",
        "    self.img_size = img_size\n",
        "    self.device = device\n",
        "\n",
        "    self.beta = self.prepare_noise_schedule().to(device)\n",
        "    self.alpha = 1-self.beta\n",
        "    self.alpha_hat = torch.cumprod(self.alpha,dim =0)\n",
        "  def prepare_noise_schedule(self):\n",
        "    return torch.linspace(self.beta_start,self.beta_end,self.noise_step)\n",
        "\n",
        "  def noise_images(self,x,t):\n",
        "    '''\n",
        "    Function which creates noise in images. [FORWARDS DIFFUSION PROCESS]\n",
        "    Instead of adding noise at each timestep, we can directly reach at final timestep (t)\n",
        "\n",
        "    Returns : sqrt(alpha_hat) * X + sqrt(1-alpha_hat) * noise and noise\n",
        "    '''\n",
        "    sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None,None,None]\n",
        "    sqrt_one_minus_alpha_hat = torch.sqrt(1.-self.alpha_hat[t])[:,None,None,None]\n",
        "    e = torch.rand_like(x)\n",
        "    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * e , e\n",
        "\n",
        "  def sample(self,model,n):\n",
        "    logging.info(f\"Sampling {n} new images...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      x = torch.randn((n,3,self.img_size,self.img_size)).to(self.device)\n",
        "      for i in tqdm(reversed(range(1,self.noise_steps)),position = 0):\n",
        "        t = (torch.ones(n) * i).long().to(self.device)\n",
        "        predicted_noise = model(x,t)\n",
        "        alpha = self.alpha[t][:,None,None,None]\n",
        "        alpha_hat = self.alpha_hat[t][:,None,None,None]\n",
        "        beta = self.beta[t][:,None,None,None]\n",
        "        if(i>1):\n",
        "          # Noise of each timestep ; final timestep will be clear image hence no noise\n",
        "          noise  =torch.randn_like(x)\n",
        "        else:\n",
        "          noise = torch.zeros_like(x)\n",
        "        x = 1/torch.sqrt(alpha) * (x - ((1-alpha) / (torch.sqrt(1-alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "    model.train()\n",
        "    x = (x.clamp(-1,1) + 1) / 2\n",
        "    x = (x * 255).type(torch.uint8)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F8PepobWFrid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  '''\n",
        "  Constructs UNet Architecture which is used in the Original Paper\n",
        "  Uses Attention  and Conv blocks to prepare the encoder - decoder type structure of the Diffusion Model\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self,c_in = 3,c_out = 3,time_dim = 256,device = \"cuda\"):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.time_dim  = time_dim\n",
        "    self.inc = DoubleConv(c_in,64)\n",
        "    self.down1 = Down(64,128)\n",
        "    self.sa2 = SelfAttention(128,32)\n",
        "    self.down2 = Down(128,256)\n",
        "    self.sa2 = SelfAttention(256,16)\n",
        "    self.down3 = Down(256,256)\n",
        "    self.sa3 = SelfAttention(256,8)\n",
        "\n",
        "    self.bot1 = DoubeConv(256,512)\n",
        "    self.bot2 = DoubeConv(512,512)\n",
        "    self.bot3 = DoubleConv(512,256)\n",
        "\n",
        "    self.up1 = Up(512,128)\n",
        "    self.sa4 = SelfAttention(128,16)\n",
        "    self.up2 = Up(256,64)\n",
        "    self.sa5 = SelfAttention(64,32)\n",
        "    self.up3 = Up(128,64)\n",
        "    self.sa6 = SelfAttention(64,64)\n",
        "    self.outc = nn.Conv2d(64,c_out,kernel_size = 1)\n",
        "  def pos_encoding(self,t,channels):\n",
        "    '''\n",
        "    9:28\n",
        "    '''\n",
        "    inv_freq = 1.0/(10000 ** (torch.arrange(0,channels,2,device = self.device).float()/channels))\n",
        "    pos_enc_a = torch.sin(t.repeat(1,channels//2) * inv_freq)\n",
        "    pos_enc_b = torch.cos(t.repeat(1,channels//2) * inv_freq)\n",
        "    pos_enc = torch.cat([pos_enc_a,pos_enc_b],dim = 1)\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self,x,t):\n",
        "    t = t.unsqueeze(-1).type(torch.float)\n",
        "    t = self.pos_encoding(t,self.time_dim)\n",
        "\n",
        "    x1 = self.inc1(x)\n",
        "    x2 = self.down(x1,t)\n",
        "    x2 = self.sa1(x2)\n",
        "\n",
        "    x3 = self.down2(x2,t)\n",
        "    x3 = self.sa2(x3)\n",
        "\n",
        "    x4 = self.down3(x3,t)\n",
        "    x4 = self.sa3(x4)\n",
        "\n",
        "    x4 = self.bot1(x4)\n",
        "    x4 = self.bot2(x4)\n",
        "    x4 = self.bot3(x4)\n",
        "\n",
        "    x = self.up1(x4,x3,t)\n",
        "    x = self.sa4(x)\n",
        "    x = self.up2(x,x2,t)\n",
        "    x = self.sa5(x)\n",
        "    x = self.up3(x,x1,t)\n",
        "    x = self.sa6(x)\n",
        "    output = self.outc(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6mdf0cGaQrPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,mid_channels= None, residual = False):\n",
        "    super().__init__()\n",
        "    self.residual = residual\n",
        "    if not mid_channels :\n",
        "      mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2D(in_channels,mid_channels,kernel_size = 3, padding = 1, bias = False)\n",
        "        nn.GroupNorm(1,mid_channels)\n",
        "        nn.GELU()\n",
        "        nn.Conv2D(mid_channels,out_channels,kernel_size = 3, padding = 1, bias = False)\n",
        "        nn.GroupNorm(1,out_channels),\n",
        "    )\n",
        "    def forward(self,x):\n",
        "      if self.residual:\n",
        "        return F.gelu(x+ self.double_conv(x))\n",
        "      else:\n",
        "        return self.double_conv(x)"
      ],
      "metadata": {
        "id": "0EqIVMH6QrRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,emb_dim=256):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv= nn.Sequential(\n",
        "        nn.MaxPool2D(2)\n",
        "        DoubleConv(in_channels,in_channels,residual = True)\n",
        "        DoubleConv(in_channels,out_channels,residual = False)\n",
        "    )\n",
        "    self.emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(emb_dim,out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x,t):\n",
        "    x = self.maxpool_conv(x)\n",
        "    emb = self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
        "    return x+ emb\n",
        "\n"
      ],
      "metadata": {
        "id": "NuLwgFPFQrTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,emb_dim=256):\n",
        "    super().__init__()\n",
        "    self.up = nn.Upsample(scale_factor = 2,mode = 'bilinear',align_corners = True)\n",
        "\n",
        "    self.conv= nn.Sequential(\n",
        "\n",
        "        DoubleConv(in_channels,in_channels,residual = True)\n",
        "        DoubleConv(in_channels,out_channels,,in_channels //2,residual = False)\n",
        "    )\n",
        "    self.emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(emb_dim,out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x,skip_x,t):\n",
        "    x = self.up(x)\n",
        "    x = torch.cat([skip_x,x],dim = 1)\n",
        "    x = self.conv(x)\n",
        "    emb = self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
        "    return x+ emb\n"
      ],
      "metadata": {
        "id": "JjApJz15QrVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  '''\n",
        "  Self Attention Module\n",
        "  '''\n",
        "  def __init__(self,channels,size):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    self.channels = channels\n",
        "    self.size = size\n",
        "    self.mha = MultiheadAttention(channels,4,batch_first = True)\n",
        "    self.ln = nn.LayerNorm([channels])\n",
        "    self.ff_self = nn.Sequential(\n",
        "        nn.LayerNorm([channels]),\n",
        "        nn.Linear(channels,channels),\n",
        "        nn.GELU()\n",
        "        nn.Linear(channels,channels),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    Forward Pass for the attention Layer\n",
        "    Note: Attention works better if axes are swapped.\n",
        "          For eg. its better to change an array [1,128,32,32]  to [1,1024,128]\n",
        "                  swap it back after attention value has been computed\n",
        "    '''\n",
        "    x = x.view(-1,self.channels,self.size * self.size).swapaxes(1,2)\n",
        "    x_ln = self.ln(x)\n",
        "    attention_value,_=self.mha(x_ln,x_ln,x_ln)\n",
        "    attention_value = attention_value + x\n",
        "    attention_value = self.ff_self(attention_value) + attention_value\n",
        "    return attention_value.swapaxes(2,1).view(-1,self.channels,self.size,self.size)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "iFvoqA13QrYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create some Util Functions"
      ],
      "metadata": {
        "id": "41_I_534Wsao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images):\n",
        "  '''\n",
        "  Util Function to plot images.\n",
        "  '''\n",
        "  # define plot size\n",
        "  plt.figure(figsize = (32,32))\n",
        "\n",
        "  # concat a bunch of images\n",
        "  torch.cat([\n",
        "      torch.cat([i for i in images.cpu()],dim = -1)\n",
        "  ],dim = -2).permute(1,2,0).cpu()\n",
        "  plt.show()\n",
        "\n",
        "def save_images(images, image_path,**kwargs):\n",
        "  grid = torchvision.make_grid(images,**kwargs)\n",
        "  ndarr = grid.permute(1,2,0).to('cpu').numpy()\n",
        "  im = Image.fromarray(ndarr)\n",
        "  im.save(path)\n",
        "\n",
        "def get_data(args):\n",
        "  '''\n",
        "  Prepare data by applying transforms\n",
        "  1. Resize images to 80%\n",
        "  2. create random cropped images\n",
        "  3. normalize images\n",
        "  '''\n",
        "  transforms = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.Resize(80),\n",
        "      torchvision.transforms.RandomResizedCrop(args.img_size,scale = (0.8,1.0)),\n",
        "      torchvision.tranforms.ToTensor(),\n",
        "      torchvision.tranforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "  ])\n",
        "\n",
        "  dataset = torchvision.datasets.ImageFolder(args.dataset_path , transform = transforms)\n",
        "  dataloader = DataLoader(dataset,batch_size = args.batch_size,shuffle = True)\n",
        "  return dataloader\n",
        "\n",
        "def logger(run_name):\n",
        "  os.makedirs(\"models\",exist_ok = True)\n",
        "  os.makedirs(\"results\",exist_ok = True)\n",
        "  os.makedirs(os.path.join(\"models\",run_name),exist_ok = True)\n",
        "  os.makedirs(os.path.join(\"results\",run_name),exist_ok = True)"
      ],
      "metadata": {
        "id": "lG1K3fihWo0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "  logger(args.run_name)\n",
        "  device = args.device\n",
        "  dataloader=get_data(args)\n",
        "  model = UNet().to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(),lr = args.lr)\n",
        "  mse = nn.MSELoss()\n",
        "  diffusion = Diffusion(img_size = args.image_size, device = device)\n",
        "  logger = SummaryWriter()\n",
        "  '''\n",
        "  14:13\n",
        "  https://www.youtube.com/watch?v=TBCRlnwJtZU\n",
        "  '''"
      ],
      "metadata": {
        "id": "h4Qwe0n_Wo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkqbpwQaWo5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waQ1-t6KWo94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}