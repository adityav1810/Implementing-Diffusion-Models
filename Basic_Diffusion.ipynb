{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNPLiT3GNDmPrzBr9fjcHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityav1810/Implementing-Diffusion-Models/blob/main/Basic_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TFXGQeG0E554"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjsUOt866_6_",
        "outputId": "83e81329-2ac6-4277-8229-120d25fca384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format = \"%(asctime)s - %(levelname)s: %(message)s\",level = logging.INFO,datefmt=\"%I:%M:%S\")"
      ],
      "metadata": {
        "id": "tlKimiHrFTT3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion:\n",
        "  '''\n",
        "  Class to setup utils and necessary functions needed to build a basic diffusion model\n",
        "  Params are the same as presented in the original paper(https://arxiv.org/abs/2006.11239)\n",
        "\n",
        "  '''\n",
        "  def __init__(self,noise_steps = 1000,beta_start = 1e-4,beta_end = 0.02,img_size = 256,device=\"cuda\"):\n",
        "    '''\n",
        "    Initialise variables generate noise\n",
        "    beta is used to generate noise; alpha = 1-beta\n",
        "\n",
        "    '''\n",
        "    self.noise_steps = noise_steps\n",
        "    self.beta_start = beta_start\n",
        "    self.beta_end = beta_end\n",
        "    self.img_size = img_size\n",
        "    self.device = device\n",
        "\n",
        "    self.beta = self.prepare_noise_schedule().to(device)\n",
        "    self.alpha = 1-self.beta\n",
        "    self.alpha_hat = torch.cumprod(self.alpha,dim =0)\n",
        "  def prepare_noise_schedule(self):\n",
        "    return torch.linspace(self.beta_start,self.beta_end,self.noise_steps)\n",
        "\n",
        "  def noise_images(self,x,t):\n",
        "    '''\n",
        "    Function which creates noise in images. [FORWARDS DIFFUSION PROCESS]\n",
        "    Instead of adding noise at each timestep, we can directly reach at final timestep (t)\n",
        "\n",
        "    Returns : sqrt(alpha_hat) * X + sqrt(1-alpha_hat) * noise and noise\n",
        "    '''\n",
        "    sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None,None,None]\n",
        "    sqrt_one_minus_alpha_hat = torch.sqrt(1.-self.alpha_hat[t])[:,None,None,None]\n",
        "    e = torch.rand_like(x)\n",
        "    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * e , e\n",
        "\n",
        "  def sample_timesteps(self,n):\n",
        "    return torch.randint(low = 1, high = self.noise_steps,size = (n,))\n",
        "\n",
        "  def sample(self,model,n):\n",
        "    logging.info(f\"Sampling {n} new images...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      x = torch.randn((n,3,self.img_size,self.img_size)).to(self.device)\n",
        "      for i in tqdm(reversed(range(1,self.noise_steps)),position = 0):\n",
        "        t = (torch.ones(n) * i).long().to(self.device)\n",
        "        predicted_noise = model(x,t)\n",
        "        alpha = self.alpha[t][:,None,None,None]\n",
        "        alpha_hat = self.alpha_hat[t][:,None,None,None]\n",
        "        beta = self.beta[t][:,None,None,None]\n",
        "        if(i>1):\n",
        "          # Noise of each timestep ; final timestep will be clear image hence no noise\n",
        "          noise  =torch.randn_like(x)\n",
        "        else:\n",
        "          noise = torch.zeros_like(x)\n",
        "        x = 1/torch.sqrt(alpha) * (x - ((1-alpha) / (torch.sqrt(1-alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "    model.train()\n",
        "    x = (x.clamp(-1,1) + 1) / 2\n",
        "    x = (x * 255).type(torch.uint8)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F8PepobWFrid"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  '''\n",
        "  Constructs UNet Architecture which is used in the Original Paper\n",
        "  Uses Attention  and Conv blocks to prepare the encoder - decoder type structure of the Diffusion Model\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
        "          super().__init__()\n",
        "          self.device = device\n",
        "          self.time_dim = time_dim\n",
        "          self.inc = DoubleConv(c_in, 64)\n",
        "          self.down1 = Down(64, 128)\n",
        "          self.sa1 = SelfAttention(128, 32)\n",
        "          self.down2 = Down(128, 256)\n",
        "          self.sa2 = SelfAttention(256, 16)\n",
        "          self.down3 = Down(256, 256)\n",
        "          self.sa3 = SelfAttention(256, 8)\n",
        "\n",
        "          self.bot1 = DoubleConv(256, 512)\n",
        "          self.bot2 = DoubleConv(512, 512)\n",
        "          self.bot3 = DoubleConv(512, 256)\n",
        "\n",
        "          self.up1 = Up(512, 128)\n",
        "          self.sa4 = SelfAttention(128, 16)\n",
        "          self.up2 = Up(256, 64)\n",
        "          self.sa5 = SelfAttention(64, 32)\n",
        "          self.up3 = Up(128, 64)\n",
        "          self.sa6 = SelfAttention(64, 64)\n",
        "          self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
        "  def pos_encoding(self,t,channels):\n",
        "    '''\n",
        "    9:28\n",
        "    '''\n",
        "    inv_freq = 1.0/(10000 ** (torch.arange(0,channels,2,device = self.device).float()/channels))\n",
        "    pos_enc_a = torch.sin(t.repeat(1,channels//2) * inv_freq)\n",
        "    pos_enc_b = torch.cos(t.repeat(1,channels//2) * inv_freq)\n",
        "    pos_enc = torch.cat([pos_enc_a,pos_enc_b],dim = 1)\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self,x,t):\n",
        "    t = t.unsqueeze(-1).type(torch.float)\n",
        "    t = self.pos_encoding(t,self.time_dim)\n",
        "\n",
        "    x1 = self.inc(x)\n",
        "    x2 = self.down1(x1,t)\n",
        "    x2 = self.sa1(x2)\n",
        "\n",
        "    x3 = self.down2(x2,t)\n",
        "    x3 = self.sa2(x3)\n",
        "\n",
        "    x4 = self.down3(x3,t)\n",
        "    x4 = self.sa3(x4)\n",
        "\n",
        "    x4 = self.bot1(x4)\n",
        "    x4 = self.bot2(x4)\n",
        "    x4 = self.bot3(x4)\n",
        "\n",
        "    x = self.up1(x4,x3,t)\n",
        "    x = self.sa4(x)\n",
        "    x = self.up2(x,x2,t)\n",
        "    x = self.sa5(x)\n",
        "    x = self.up3(x,x1,t)\n",
        "    x = self.sa6(x)\n",
        "    output = self.outc(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6mdf0cGaQrPE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,mid_channels= None, residual = False):\n",
        "    super().__init__()\n",
        "    self.residual = residual\n",
        "    if not mid_channels :\n",
        "      mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,mid_channels,kernel_size = 3, padding = 1, bias = False),\n",
        "        nn.GroupNorm(1,mid_channels),\n",
        "        nn.GELU(),\n",
        "        nn.Conv2d(mid_channels,out_channels,kernel_size = 3, padding = 1, bias = False),\n",
        "        nn.GroupNorm(1,out_channels),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    if self.residual:\n",
        "      return F.gelu(x+ self.double_conv(x))\n",
        "    else:\n",
        "      return self.double_conv(x)"
      ],
      "metadata": {
        "id": "0EqIVMH6QrRk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,emb_dim=256):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv= nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels,in_channels,residual = True),\n",
        "        DoubleConv(in_channels,out_channels,residual = False),\n",
        "    )\n",
        "    self.emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(emb_dim,out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n"
      ],
      "metadata": {
        "id": "NuLwgFPFQrTf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,emb_dim=256):\n",
        "    super().__init__()\n",
        "    self.up = nn.Upsample(scale_factor = 2,mode = 'bilinear',align_corners = True)\n",
        "\n",
        "    self.conv= nn.Sequential(\n",
        "\n",
        "        DoubleConv(in_channels,in_channels,residual = True),\n",
        "        DoubleConv(in_channels,out_channels,in_channels //2,residual = False),\n",
        "    )\n",
        "    self.emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(emb_dim,out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x,skip_x,t):\n",
        "    x = self.up(x)\n",
        "    x = torch.cat([skip_x,x],dim = 1)\n",
        "    x = self.conv(x)\n",
        "    emb = self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
        "    return x+ emb\n"
      ],
      "metadata": {
        "id": "JjApJz15QrVx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  '''\n",
        "  Self Attention Module\n",
        "  '''\n",
        "  def __init__(self,channels,size):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    self.channels = channels\n",
        "    self.size = size\n",
        "    self.mha = torch.nn.MultiheadAttention(channels,4,batch_first = True)\n",
        "    self.ln = nn.LayerNorm([channels])\n",
        "    self.ff_self = nn.Sequential(\n",
        "        nn.LayerNorm([channels]),\n",
        "        nn.Linear(channels,channels),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(channels,channels),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    Forward Pass for the attention Layer\n",
        "    Note: Attention works better if axes are swapped.\n",
        "          For eg. its better to change an array [1,128,32,32]  to [1,1024,128]\n",
        "                  swap it back after attention value has been computed\n",
        "    '''\n",
        "    x = x.view(-1,self.channels,self.size * self.size).swapaxes(1,2)\n",
        "    x_ln = self.ln(x)\n",
        "    attention_value,_=self.mha(x_ln,x_ln,x_ln)\n",
        "    attention_value = attention_value + x\n",
        "    attention_value = self.ff_self(attention_value) + attention_value\n",
        "    return attention_value.swapaxes(2,1).view(-1,self.channels,self.size,self.size)\n",
        "\n"
      ],
      "metadata": {
        "id": "iFvoqA13QrYC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create some Util Functions"
      ],
      "metadata": {
        "id": "41_I_534Wsao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images):\n",
        "  '''\n",
        "  Util Function to plot images.\n",
        "  '''\n",
        "  # define plot size\n",
        "  plt.figure(figsize = (32,32))\n",
        "\n",
        "  # concat a bunch of images\n",
        "  torch.cat([\n",
        "      torch.cat([i for i in images.cpu()],dim = -1)\n",
        "  ],dim = -2).permute(1,2,0).cpu()\n",
        "  plt.show()\n",
        "\n",
        "def save_images(images, image_path,**kwargs):\n",
        "  grid = torchvision.make_grid(images,**kwargs)\n",
        "  ndarr = grid.permute(1,2,0).to('cpu').numpy()\n",
        "  im = Image.fromarray(ndarr)\n",
        "  im.save(image_path)\n",
        "\n",
        "def get_data(args):\n",
        "  '''\n",
        "  Prepare data by applying transforms\n",
        "  1. Resize images to 80%\n",
        "  2. create random cropped images\n",
        "  3. normalize images\n",
        "  '''\n",
        "  transforms = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.Resize(80),\n",
        "      torchvision.transforms.RandomResizedCrop(args.image_size,scale = (0.8,1.0)),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "  ])\n",
        "\n",
        "  # dataset = torchvision.datasets.ImageFolder(args.dataset_path , transform = transforms)\n",
        "  dataset = torchvision.datasets.CIFAR10(root =\"/content/dataset\",transform = transforms,download = True)\n",
        "  dataloader = DataLoader(dataset,batch_size = args.batch_size,shuffle = True)\n",
        "  return dataloader\n",
        "\n",
        "def setup_logger(run_name):\n",
        "  os.makedirs(\"models\",exist_ok = True)\n",
        "  os.makedirs(\"results\",exist_ok = True)\n",
        "  os.makedirs(os.path.join(\"models\",run_name),exist_ok = True)\n",
        "  os.makedirs(os.path.join(\"results\",run_name),exist_ok = True)"
      ],
      "metadata": {
        "id": "lG1K3fihWo0c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "  setup_logger(args.run_name)\n",
        "  device = args.device\n",
        "  dataloader=get_data(args)\n",
        "  model = UNet().to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(),lr = args.lr)\n",
        "  mse = nn.MSELoss()\n",
        "  diffusion = Diffusion(img_size = args.image_size, device = device)\n",
        "  logger = SummaryWriter(os.path.join(\"runs\",args.run_name))\n",
        "  l = len(dataloader)\n",
        "\n",
        "  for epoch in range(args.epochs):\n",
        "    logging.info(f\"Starting with Epoch {epoch} : \")\n",
        "    pbar = tqdm(dataloader)\n",
        "    for i ,(images,_) in enumerate(pbar):\n",
        "      images = images.to(device)\n",
        "      t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "      x_t ,noise = diffusion.noise_images(images,t)\n",
        "      predicted_noise = model(x_t,t)\n",
        "      loss = mse(predicted_noise,noise)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pbar.set_postfix(MSE=loss.item())\n",
        "      logger.add_scaler(\"MSE\",loss.item(),global_step = epoch*l + i)\n",
        "    sampled_images = diffusion.sample(model,n = images.shape[0])\n",
        "    save_images(sampled_images,os.path.join(\"results\",args.run_name,f\"{epoch}.jpg\"))\n",
        "    torch.save(model.start_dict(),os.path.join(\"models\",args.run_name,f\"chkpt.pt\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h4Qwe0n_Wo2_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "args = parser.parse_args()\n",
        "args.run_name = \"DDPM_Unconditional\"\n",
        "args.epochs = 500\n",
        "args.batch_size = 12\n",
        "args.image_size = 64\n",
        "\n",
        "args.device = \"cuda\"\n",
        "args.lr = 3e-4\n"
      ],
      "metadata": {
        "id": "NkqbpwQaWo5p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(args)"
      ],
      "metadata": {
        "id": "waQ1-t6KWo94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "efa2681c-77ad-4a6b-b9ab-b7b3c5662714"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4167 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aad596905035>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-89027d281541>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_timesteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mx_t\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-709fcf6d6267>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a8a762890f40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (12) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5NkPM7x7zuo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSgUCKgqAMif"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}